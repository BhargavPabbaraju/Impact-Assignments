{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b85485-4c5e-479d-bbe0-dd76f53ab591",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "**Min-Max scaling**, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range, usually between 0 and 1. This process transforms the original values of features so that they have the same scale, making them suitable for algorithms that are sensitive to the scale of input features, such as gradient descent-based optimization algorithms or distance-based metrics.\n",
    "\n",
    "Min-Max scaling is performed using the following formula:\n",
    "\n",
    "$ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $\n",
    "\n",
    "Where:\n",
    "- $ X $ is the original feature value.\n",
    "-$ X_{\\text{min}} $ is the minimum value of the feature in the dataset.\n",
    "- $ X_{\\text{max}} $ is the maximum value of the feature in the dataset.\n",
    "\n",
    "This transformation ensures that the feature values are mapped to the range [0, 1].\n",
    "\n",
    "**Example: Min-Max Scaling**\n",
    "\n",
    "Suppose you have a dataset of house prices with the \"size\" feature representing the size of houses in square feet. The original \"size\" values range from 800 to 3000 square feet.\n",
    "\n",
    "Original \"size\" values: $800, 1200, 1500, 2000, 2500, 3000$\n",
    "\n",
    "Applying Min-Max scaling:\n",
    "\n",
    "1. Find the minimum and maximum values of the \"size\" feature:\n",
    "   - $ X_{\\text{min}} = 800 $\n",
    "   - $ X_{\\text{max}} = 3000 $\n",
    "\n",
    "2. Calculate the scaled values using the formula for each original value:\n",
    "   - For $ X = 800 $: $ X_{\\text{scaled}} = \\frac{800 - 800}{3000 - 800} = 0$\n",
    "   - For $ X = 1200 $: $ X_{\\text{scaled}} = \\frac{1200 - 800}{3000 - 800} \\approx 0.1429 $\n",
    "   - And so on for other values.\n",
    "\n",
    "The resulting scaled \"size\" values will be between 0 and 1, representing the scaled feature suitable for feeding into machine learning algorithms.\n",
    "\n",
    "Scaled \"size\" values: $0, 0.1429, 0.2857, 0.5714, 0.8571, 1$\n",
    "\n",
    "Min-Max scaling ensures that the \"size\" feature values are normalized and consistent in scale, preventing features with larger numerical values from dominating the learning process and making the algorithm more effective in handling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935378f9-c69e-432f-a4b8-b9b0384f21a4",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The **Unit Vector technique**, also known as **Normalization**, is another method of scaling numerical features in data preprocessing. Unlike Min-Max scaling, which scales the features to a specific range (usually between 0 and 1), the Unit Vector technique scales the feature vectors to have a magnitude of 1 while preserving their direction. This can be particularly useful when the magnitude of the feature values is not as important as their direction or when dealing with algorithms that are sensitive to the scale and direction of features, such as clustering algorithms.\n",
    "\n",
    "The Unit Vector technique is performed using the following formula:\n",
    "\n",
    "$ X_{\\text{normalized}} = \\frac{X}{\\|X\\|} $\n",
    "\n",
    "Where:\n",
    "- $ X $ is the original feature vector.\n",
    "- $ \\|X\\| $ is the Euclidean norm (magnitude) of the feature vector.\n",
    "\n",
    "**Example: Unit Vector Scaling**\n",
    "\n",
    "Suppose you have a dataset of house prices with two features: \"size\" representing the size of houses in square feet, and \"price\" representing the price of houses in dollars. The original feature vectors have the following values:\n",
    "\n",
    "Original feature vectors:\n",
    "$ (800, 100000), (1200, 150000), (1500, 200000), (2000, 250000), (2500, 300000), (3000, 350000) $\n",
    "\n",
    "Applying the Unit Vector technique:\n",
    "\n",
    "1. Calculate the Euclidean norm (magnitude) of each feature vector.\n",
    "   - For the first vector: $ \\|X\\| = \\sqrt{800^2 + 100000^2} \\approx 100062.185 $\n",
    "\n",
    "2. Divide each feature vector by its magnitude to obtain the normalized feature vectors.\n",
    "   - For the first vector: $ (800, 100000)_{\\text{normalized}} = \\frac{(800, 100000)}{100062.185} \\approx (0.007998, 0.999968) $\n",
    "\n",
    "Repeat the normalization process for all feature vectors.\n",
    "\n",
    "Normalized feature vectors:\n",
    "$ (0.007998, 0.999968), (0.007996, 0.999968), (0.007499, 0.999971), (0.007997, 0.999968), (0.008332, 0.999966), (0.008571, 0.999964) $\n",
    "\n",
    "In this example, the Unit Vector technique scales the feature vectors to have a magnitude of 1 while preserving their direction. This normalization is particularly useful when you're interested in the relationships between the directions of features, and the magnitude of individual feature values is not as relevant.\n",
    "\n",
    "In summary, while both Min-Max scaling and the Unit Vector technique are used for feature scaling, they serve different purposes. Min-Max scaling focuses on scaling the values to a specific range, while the Unit Vector technique normalizes the vectors' magnitudes to 1 while maintaining their direction. The choice between these methods depends on the specific characteristics of your data and the requirements of the algorithm you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869eb3e-fdd7-4004-8155-0edd8013dd5a",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information and minimizing the loss of variance. PCA achieves this by identifying the principal components, which are linear combinations of the original features, that capture the maximum variance in the data. These principal components are orthogonal to each other, meaning they are uncorrelated, and they can be ranked in terms of the amount of variance they explain.\n",
    "\n",
    "PCA is commonly used in various fields, such as image processing, data compression, and feature engineering, to reduce the complexity of the data while retaining as much information as possible.\n",
    "\n",
    "**Steps in PCA:**\n",
    "\n",
    "1. **Standardization:** Standardize the features to have a mean of 0 and a standard deviation of 1. This ensures that features with different scales don't dominate the PCA process.\n",
    "\n",
    "2. **Calculate Covariance Matrix:** Calculate the covariance matrix of the standardized features to understand the relationships between them.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:** Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) of maximum variance, while eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Sort Eigenvectors:** Sort the eigenvectors in descending order of their corresponding eigenvalues. The eigenvectors with the largest eigenvalues (most variance) are the most important and are selected as the principal components.\n",
    "\n",
    "5. **Project Data:** Transform the original data onto the new lower-dimensional space defined by the selected principal components. This is done by computing the dot product of the data and the selected eigenvectors.\n",
    "\n",
    "**Example: PCA in Dimensionality Reduction**\n",
    "\n",
    "Suppose you have a dataset of 2D points representing the height and weight of individuals. You want to reduce the dimensionality while retaining the most important information.\n",
    "\n",
    "Original data points:\n",
    "```\n",
    "Height (in inches)  |  Weight (in pounds)\n",
    "---------------------------------------\n",
    "  60                |       110\n",
    "  64                |       140\n",
    "  68                |       160\n",
    "  69                |       150\n",
    "  70                |       180\n",
    "```\n",
    "\n",
    "1. **Standardization:** Standardize the height and weight values.\n",
    "\n",
    "2. **Calculate Covariance Matrix:** Calculate the covariance matrix:\n",
    "```\n",
    "         Height    Weight\n",
    "Height   2.56      25.0\n",
    "Weight   25.0      400.0\n",
    "```\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:** Calculate eigenvectors and eigenvalues:\n",
    "   - Eigenvector 1: [0.998, -0.063]\n",
    "   - Eigenvector 2: [0.063, 0.998]\n",
    "   - Eigenvalue 1: 400.192\n",
    "   - Eigenvalue 2: 1.407\n",
    "\n",
    "4. **Sort Eigenvectors:** Sort eigenvectors by eigenvalues. Choose the top eigenvector (most variance).\n",
    "\n",
    "5. **Project Data:** Project the data onto the first principal component:\n",
    "```\n",
    "Projected Data = Data x Eigenvector 1\n",
    "```\n",
    "\n",
    "The projected data points represent the transformed lower-dimensional space, capturing the most important information.\n",
    "\n",
    "PCA allows you to reduce the original 2D data to a 1D representation along the direction of maximum variance, while still retaining a significant portion of the variance. This is useful for visualizations, reducing computational complexity, and improving model performance when dealing with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc32f1-6103-40de-a8bd-632bd2e11430",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**PCA (Principal Component Analysis)** and **Feature Extraction** are closely related concepts. PCA can be used as a technique for feature extraction, where it transforms the original features into a new set of features that capture the most important information and reduce the dimensionality of the data. Feature extraction aims to create a more compact representation of the data by selecting a subset of features or creating new features that are a combination of the original ones.\n",
    "\n",
    "**Steps for Using PCA for Feature Extraction:**\n",
    "\n",
    "1. **Standardization:** Standardize the features to have a mean of 0 and a standard deviation of 1. This step is crucial to ensure that features with different scales don't dominate the PCA process.\n",
    "\n",
    "2. **Calculate Covariance Matrix:** Calculate the covariance matrix of the standardized features to understand the relationships between them.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:** Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) of maximum variance, while eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Sort Eigenvectors:** Sort the eigenvectors in descending order of their corresponding eigenvalues. The eigenvectors with the largest eigenvalues (most variance) are the most important and can be selected as the basis for feature extraction.\n",
    "\n",
    "5. **Select Eigenvectors:** Choose the top $ k $ eigenvectors that correspond to the highest eigenvalues, where $ k$ is the desired number of new features.\n",
    "\n",
    "6. **Project Data:** Transform the original data onto the new lower-dimensional space defined by the selected $ k $eigenvectors. This is done by computing the dot product of the data and the selected eigenvectors.\n",
    "\n",
    "**Example: PCA for Feature Extraction**\n",
    "\n",
    "Suppose you have a dataset of grayscale images of handwritten digits, where each image has $ 28 \\times 28 $ pixels (a total of 784 features). You want to reduce the dimensionality of the images while retaining the most important information for digit recognition.\n",
    "\n",
    "1. **Standardization:** Standardize the pixel values to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Calculate Covariance Matrix:** Calculate the covariance matrix of the standardized pixel values.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:** Calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "4. **Sort Eigenvectors:** Sort eigenvectors by eigenvalues in descending order.\n",
    "\n",
    "5. **Select Eigenvectors:** Choose the top $ k $ eigenvectors that explain the most variance. These eigenvectors represent the most significant patterns in the data.\n",
    "\n",
    "6. **Project Data:** Project the original images onto the lower-dimensional space defined by the selected eigenvectors.\n",
    "\n",
    "By selecting a reduced number of eigenvectors (features), you've effectively extracted the most informative patterns from the images. These new features can be used for digit recognition tasks, reducing computational complexity, and improving model performance.\n",
    "\n",
    "Using PCA for feature extraction allows you to represent the data in a more compact and informative way, which is particularly useful when dealing with high-dimensional datasets, images, or other complex data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b9be5-3269-4132-84e5-d573470b3b92",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess the numerical features such as price, rating, and delivery time. Min-Max scaling will ensure that these features are within a specific range (usually between 0 and 1), making them suitable for various machine learning algorithms, including recommendation systems. Here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "**Step 1: Understand the Data:**\n",
    "Start by understanding the features you have in the dataset, such as price, rating, and delivery time. Know their distributions and ranges to determine if scaling is necessary.\n",
    "\n",
    "**Step 2: Apply Min-Max Scaling:**\n",
    "Min-Max scaling transforms the original feature values $X$ into scaled values $X_{\\text{scaled}}$ using the formula:\n",
    "\n",
    "$ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} $\n",
    "\n",
    "Where:\n",
    "- $X$ is the original feature value.\n",
    "- $X_{\\text{min}}$ is the minimum value of the feature.\n",
    "- $X_{\\text{max}}$ is the maximum value of the feature.\n",
    "\n",
    "**Step 3: Implement Min-Max Scaling:**\n",
    "For each feature (price, rating, delivery time), apply the Min-Max scaling formula to transform the values to the desired range (usually between 0 and 1).\n",
    "\n",
    "**Example: Min-Max Scaling for Recommendation System**\n",
    "\n",
    "Suppose you have the following example data for the features:\n",
    "\n",
    "- Price (range: $5 to $25)\n",
    "- Rating (range: 2 to 5)\n",
    "- Delivery Time (range: 15 to 60 minutes)\n",
    "\n",
    "1. Calculate $X_{\\text{min}}$ and $X_{\\text{max}}$ for each feature.\n",
    "\n",
    "   - For Price: $X_{\\text{min}} = 5$,$X_{\\text{max}} = 25$\n",
    "   - For Rating: $X_{\\text{min}} = 2$, $X_{\\text{max}} = 5$\n",
    "   - For Delivery Time: $X_{\\text{min}} = 15$, $X_{\\text{max}} = 60$\n",
    "\n",
    "2. Apply Min-Max scaling to transform the values for each feature:\n",
    "\n",
    "   - For Price: $X_{\\text{scaled}} = \\frac{X - 5}{25 - 5}$\n",
    "   - For Rating: $X_{\\text{scaled}} = \\frac{X - 2}{5 - 2}$\n",
    "   - For Delivery Time: $X_{\\text{scaled}} = \\frac{X - 15}{60 - 15}$\n",
    "\n",
    "After Min-Max scaling, the features will be transformed into the desired range between 0 and 1, making them suitable for use in a recommendation system.\n",
    "\n",
    "Using Min-Max scaling in preprocessing ensures that all features contribute equally to the recommendation system and prevents features with larger numerical values from dominating the recommendation process. It's important to note that you need to apply the same scaling factors to new, unseen data during inference to maintain consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4bbcb-5ce1-495b-8c80-4c54a54b2281",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset in a stock price prediction project can help improve computational efficiency, mitigate the curse of dimensionality, and extract the most relevant information from the features. Here's how you would use PCA for dimensionality reduction in this context:\n",
    "\n",
    "**Step 1: Data Preparation:**\n",
    "Start by gathering the dataset containing various features related to company financial data and market trends. Ensure that the data is preprocessed, including handling missing values, standardizing the features, and encoding categorical variables if necessary.\n",
    "\n",
    "**Step 2: Standardization:**\n",
    "Standardize the features to have zero mean and unit variance. This step is crucial to ensure that PCA gives equal importance to all features regardless of their scales.\n",
    "\n",
    "**Step 3: Covariance Matrix and Eigenvalues/Eigenvectors:**\n",
    "Calculate the covariance matrix of the standardized features. Compute the eigenvectors and eigenvalues of this covariance matrix. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate how much variance is explained by each eigenvector.\n",
    "\n",
    "**Step 4: Selecting Principal Components:**\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the highest eigenvalues capture the most variance in the data and are the most informative. Decide how many principal components you want to keep based on how much variance you want to retain.\n",
    "\n",
    "**Step 5: Projecting Data onto Principal Components:**\n",
    "Create a transformation matrix using the selected top principal components. Project the original data onto these principal components to obtain a new lower-dimensional representation of the data.\n",
    "\n",
    "**Example: PCA for Stock Price Prediction**\n",
    "\n",
    "Suppose your dataset contains various financial metrics like revenue, earnings, debt-to-equity ratio, and market trend indicators like trading volume and sentiment score. These features may be high-dimensional, and using all of them could lead to overfitting or computational challenges. Here's how you could use PCA:\n",
    "\n",
    "1. **Standardization:** Standardize the financial and market trend features to ensure they have zero mean and unit variance.\n",
    "\n",
    "2. **Covariance Matrix and Eigenvalues/Eigenvectors:** Calculate the covariance matrix and find the eigenvectors and eigenvalues.\n",
    "\n",
    "3. **Selecting Principal Components:** Sort the eigenvectors based on their eigenvalues and choose the top \\(k\\) eigenvectors. The cumulative explained variance can help you decide on the number of components to retain.\n",
    "\n",
    "4. **Projecting Data:** Create a transformation matrix using the selected principal components. Project the original data onto these components to obtain a lower-dimensional representation.\n",
    "\n",
    "By using PCA, you've effectively reduced the dimensionality of the dataset while retaining the most significant information. This lower-dimensional representation can then be used as input for machine learning models to predict stock prices. It's important to note that while PCA reduces dimensionality, it might also reduce interpretability, so consider balancing these factors based on your project's goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cffcf-0838-47fb-925c-78bb79102540",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dc680c-0692-4834-90ef-ad044b8f5cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = [1,5,10,15,20]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "scaler.fit_transform(np.array(data).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bf916-5ed9-48b3-8b9e-0e04eb6dd3f6",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ec4a05-eca8-471a-abad-8ed0f322c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [7.95723269e-01 2.02589723e-01 1.68700768e-03 5.56557225e-34\n",
      " 3.52423772e-35]\n",
      "Cumulative explained variance: [0.79572327 0.99831299 1.         1.         1.        ]\n",
      "Number of components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset: [height, weight, age, gender, blood pressure]\n",
    "data = np.array([\n",
    "    [170, 65, 30, 0, 120],\n",
    "    [160, 55, 25, 1, 130],\n",
    "    [180, 75, 40, 1, 110],\n",
    "    [165, 60, 28, 0, 125],\n",
    "    [175, 70, 35, 0, 115]\n",
    "])\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of components to retain\n",
    "num_components_to_retain = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative explained variance:\", cumulative_variance)\n",
    "print(\"Number of components to retain:\", num_components_to_retain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
