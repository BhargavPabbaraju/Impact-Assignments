{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d2508e-1c76-4aac-a964-4e7c4ff72e22",
   "metadata": {},
   "source": [
    "# Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical test used to compare the means of three or more groups. To ensure the validity of the results and accurate interpretations, several assumptions need to be met when using ANOVA. These assumptions include:\n",
    "\n",
    "1. Independence of Observations: The observations in each group must be independent of each other. This means that the data points in one group should not be related or dependent on the data points in other groups.\n",
    "\n",
    "2. Normality: The data within each group should follow a normal distribution. ANOVA assumes that the populations from which the samples are drawn are normally distributed. Violation of this assumption may lead to inaccurate results and misleading conclusions.\n",
    "\n",
    "3. Homogeneity of Variance (Homoscedasticity): The variance of the dependent variable should be approximately equal across all groups. In other words, the spread of data points should be consistent in each group. Homogeneity of variance ensures that the groups have a similar level of variability. If the variance differs significantly among groups, the results of ANOVA may be unreliable.\n",
    "\n",
    "4. Homogeneity of Regression Slopes (Only for Two-Way ANOVA with Interaction): If conducting a two-way ANOVA with interaction, an additional assumption is that the slopes of the regression lines (relationship between the dependent variable and the independent variable) should be equal across groups.\n",
    "\n",
    "Examples of violations of these assumptions that could impact the validity of ANOVA results:\n",
    "\n",
    "1. Violation of Normality: If the data within each group is not normally distributed, it can lead to biased results and incorrect conclusions. For example, if the data is highly skewed or has heavy tails, ANOVA may produce inaccurate estimates of group means and standard errors.\n",
    "\n",
    "2. Violation of Homoscedasticity: If the variability in the data differs significantly among groups, the assumptions of ANOVA are not met. This can result in incorrect assessments of group differences and confidence intervals.\n",
    "\n",
    "3. Violation of Independence: If the data points within groups are not independent, such as in a repeated measures design or clustered data, it violates the assumption of independence. This can lead to underestimation or overestimation of the significance of group differences.\n",
    "\n",
    "4. Violation of Homogeneity of Regression Slopes: In a two-way ANOVA with interaction, if the slopes of the regression lines for the dependent variable and independent variables differ across groups, it can impact the interpretation of the interaction effect and the validity of the ANOVA results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f43db-6265-4ec4-9daa-4bf21b7279e2",
   "metadata": {},
   "source": [
    "# Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "The three types of ANOVA are:\n",
    "\n",
    "1. One-Way ANOVA (Analysis of Variance): One-Way ANOVA is used when we have one categorical independent variable (also called a factor) with three or more levels (groups), and we want to compare the means of the dependent variable across these groups. This test helps determine if there are any significant differences in the means of the dependent variable between the groups. One-Way ANOVA is appropriate when we have a single independent variable and want to investigate its impact on the dependent variable.\n",
    "\n",
    "Example: A researcher wants to compare the test scores of students from three different schools to see if there are any significant differences in academic performance.\n",
    "\n",
    "2. Two-Way ANOVA: Two-Way ANOVA is used when we have two categorical independent variables (factors) and one dependent variable. It allows us to examine the main effects of each independent variable separately, as well as their interaction effect (how the two independent variables together influence the dependent variable). This test is suitable when we want to investigate the combined influence of two factors on the dependent variable.\n",
    "\n",
    "Example: A researcher wants to study the effects of both gender and teaching method on the test scores of students. The independent variables are gender (male or female) and teaching method (traditional or online).\n",
    "\n",
    "3. Three-Way ANOVA: Three-Way ANOVA extends the concept of Two-Way ANOVA by adding a third categorical independent variable (factor) along with one dependent variable. It allows us to explore the main effects of each independent variable and their interaction effects in the context of three factors.\n",
    "\n",
    "Example: A researcher wants to study the effects of temperature, humidity, and light intensity on plant growth. The independent variables are temperature (low, medium, high), humidity (low, medium, high), and light intensity (low, medium, high).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e48f88-3144-49f5-92c6-9e233301ef8b",
   "metadata": {},
   "source": [
    "# Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "Partitioning of variance in ANOVA refers to the process of breaking down the total variance in the data into different components, each associated with a specific source of variation. ANOVA accomplishes this by partitioning the total variance into variance components attributable to various factors or sources of variability in the study.\n",
    "\n",
    "In a typical One-Way ANOVA, the total variance in the data is partitioned into two main components:\n",
    "\n",
    "1. Between-Groups Variance (or Treatment Variance): This component measures the variability between the group means. It represents the differences in the dependent variable caused by the independent variable (treatment) being studied. Larger between-groups variance indicates that the groups have different means.\n",
    "\n",
    "2. Within-Groups Variance (or Error Variance): This component measures the variability within each group. It represents the random fluctuations or variability in the data that cannot be explained by the independent variable. It includes individual differences, measurement error, and any other unaccounted sources of variability.\n",
    "\n",
    "The partitioning of variance allows us to assess the relative contributions of the treatment effect and random variability to the total variability in the data. By comparing the magnitude of these variance components, ANOVA helps us determine if the differences between group means (between-groups variance) are significantly larger than what we would expect due to random chance (within-groups variance).\n",
    "\n",
    "Understanding the concept of partitioning of variance is important for several reasons:\n",
    "\n",
    "1. Identifying Significant Effects: ANOVA helps us determine if the variation between groups (treatment effect) is statistically significant. If the between-groups variance is much larger than the within-groups variance, it suggests that the independent variable (treatment) has a significant effect on the dependent variable.\n",
    "\n",
    "2. Interpreting Results: By understanding the partitioning of variance, we can interpret the results of ANOVA correctly. We can infer whether the observed differences between groups are likely due to the treatment effect or simply due to random fluctuations.\n",
    "\n",
    "3. Quantifying Effect Size: The ratio of between-groups variance to total variance (also known as eta-squared) provides a measure of effect size, indicating how much of the total variability in the data can be attributed to the independent variable.\n",
    "\n",
    "4. Model Improvement: Understanding the partitioning of variance can guide researchers in refining their models and experimental designs. By understanding the sources of variability in the data, researchers can optimize the design and control factors that influence the dependent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c289a3a3-7fa4-49eb-acdd-7891d835d1c2",
   "metadata": {},
   "source": [
    "# Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44408d3d-1672-4fbc-a1f3-938c745f0337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST : 175.73333333333335\n",
      "SSE : 118.53333333333327\n",
      "SSR : 57.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "group_A = [25, 28, 32, 29, 31]\n",
    "group_B = [21, 20, 24, 23, 25]\n",
    "group_C = [27, 29, 28, 26, 30]\n",
    "\n",
    "\n",
    "data = np.concatenate([group_A, group_B, group_C])\n",
    "\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "SST = np.sum((data - overall_mean) ** 2)\n",
    "\n",
    "group_A_mean = np.mean(group_A)\n",
    "group_B_mean = np.mean(group_B)\n",
    "group_C_mean = np.mean(group_C)\n",
    "\n",
    "SSE = len(group_A) * (group_A_mean - overall_mean) ** 2 + \\\n",
    "      len(group_B) * (group_B_mean - overall_mean) ** 2 + \\\n",
    "      len(group_C) * (group_C_mean - overall_mean) ** 2\n",
    "        \n",
    "\n",
    "SSR = np.sum((group_A - group_A_mean) ** 2) + \\\n",
    "      np.sum((group_B - group_B_mean) ** 2) + \\\n",
    "      np.sum((group_C - group_C_mean) ** 2)\n",
    "\n",
    "print('SST :',SST)\n",
    "print('SSE :',SSE)\n",
    "print('SSR :',SSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1aabe3-bc41-4943-8671-44261a8c3b62",
   "metadata": {},
   "source": [
    "# Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c303ef3-d3f8-48dd-9d30-72fff53bbbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effects:\n",
      "factor1           -1.130586\n",
      "factor2            0.300133\n",
      "factor1:factor2    0.027313\n",
      "dtype: float64\n",
      "\n",
      "Interaction Effect:\n",
      "0.02731269707332209\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = {\n",
    "    'group': [1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
    "    'factor1': [10, 12, 15, 8, 9, 11, 13, 14, 16],\n",
    "    'factor2': [20, 21, 23, 18, 19, 22, 24, 25, 27]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "formula = 'factor1 + factor2 + factor1:factor2'\n",
    "\n",
    "model = ols('group ~ ' + formula, data=df).fit()\n",
    "\n",
    "main_effects = model.params.drop(['Intercept'])\n",
    "interaction_effects = model.params['factor1:factor2']\n",
    "\n",
    "print(\"Main Effects:\")\n",
    "print(main_effects)\n",
    "\n",
    "print(\"\\nInteraction Effect:\")\n",
    "print(interaction_effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2a4e7-5554-47c5-94c7-a5dbc55bd0b7",
   "metadata": {},
   "source": [
    "# Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?\n",
    "\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "1. F-statistic: The F-statistic of 5.23 represents the test statistic that compares the variance between groups to the variance within groups. A larger F-statistic indicates that there is more variability between the group means relative to the variability within the groups.\n",
    "\n",
    "2. p-value: The p-value of 0.02 represents the probability of observing an F-statistic as extreme as 5.23 under the assumption that there are no real differences between the group means (null hypothesis is true). In this case, the p-value is below the commonly used significance level of 0.05 (or 5%), suggesting that the observed differences between the groups are statistically significant.\n",
    "\n",
    "Conclusions:\n",
    "Based on the F-statistic and the p-value obtained from the one-way ANOVA:\n",
    "- We can conclude that there are significant differences between the means of the groups being compared. \n",
    "- The differences observed in the group means are unlikely to be due to random chance alone, as the p-value is less than the chosen significance level (0.05). Therefore, we reject the null hypothesis that the means of all groups are equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30c6c3-dcc7-4d6d-9ae0-38d13f82fca6",
   "metadata": {},
   "source": [
    "# Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?\n",
    "\n",
    "Handling missing data in a repeated measures ANOVA is a crucial step to ensure the validity and accuracy of the analysis. Missing data can occur when participants have incomplete responses or drop out of the study before all measurements are collected. There are several methods to handle missing data, each with its advantages and potential consequences:\n",
    "\n",
    "1. Complete Case Analysis (Listwise Deletion):\n",
    "   - This method involves excluding all participants with missing data on any variable used in the analysis. Only the complete cases are retained for the analysis.\n",
    "   - Advantage: It is straightforward to implement and does not require any additional assumptions.\n",
    "   - Consequences: Complete case analysis can lead to a reduction in sample size and may introduce bias if the missing data are related to the outcome or predictors.\n",
    "\n",
    "2. Mean Imputation:\n",
    "   - In this method, missing values are replaced with the mean value of the variable across all participants.\n",
    "   - Advantage: Mean imputation is simple and can preserve the sample size.\n",
    "   - Consequences: Mean imputation can underestimate the variability in the data, leading to an inflated Type I error rate. It may also distort the true relationships between variables, especially if missingness is related to specific subgroups.\n",
    "\n",
    "3. Last Observation Carried Forward (LOCF):\n",
    "   - LOCF involves using the last observed value for a participant for subsequent missing time points.\n",
    "   - Advantage: LOCF is simple to apply and can be appropriate when missing data occur sporadically and are likely to be missing at random.\n",
    "   - Consequences: LOCF can introduce bias if the last observed value is not a good representation of the participant's true response. It can also underestimate the variability and lead to inaccurate estimates.\n",
    "\n",
    "4. Multiple Imputation (MI):\n",
    "   - Multiple imputation creates multiple plausible imputations for the missing data based on a statistical model, and then combines the results from each imputed dataset.\n",
    "   - Advantage: MI provides valid and efficient estimates while accounting for the uncertainty in the imputation process. It yields unbiased parameter estimates and accurate standard errors.\n",
    "   - Consequences: MI can be computationally intensive and requires careful specification of the imputation model. However, it is considered a robust method for handling missing data.\n",
    "\n",
    "5. Maximum Likelihood Estimation (MLE):\n",
    "   - MLE is a statistical technique that uses all available data and estimates model parameters while accounting for the missing data mechanism.\n",
    "   - Advantage: MLE is a principled approach that provides unbiased estimates under the missing at random (MAR) assumption.\n",
    "   - Consequences: MLE may be sensitive to the specific assumptions made about the missing data mechanism, and its performance may degrade if the missing data are not missing at random.\n",
    "\n",
    "The choice of missing data handling method should be based on the underlying missing data mechanism, the amount of missingness, and the goals of the analysis. It is essential to consider the potential consequences of each method and perform sensitivity analyses to assess the robustness of the results to different missing data approaches. Additionally, researchers should be cautious in interpreting the results of the analysis and acknowledge the potential limitations associated with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15b8c37-bd0c-4606-a66e-b2c6d66eee59",
   "metadata": {},
   "source": [
    "# Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.\n",
    "\n",
    "\n",
    "Some common post-hoc tests used after ANOVA include:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD) Test:\n",
    "   - Tukey's HSD test compares all possible pairs of group means and identifies which pairs have significantly different means. It is suitable for situations where you have multiple groups and want to determine the specific differences between them.\n",
    "   - Example: Suppose you conducted a one-way ANOVA to compare the test scores of students from four different schools. The ANOVA showed a significant difference in means. To identify which schools have significantly different test scores, you can use Tukey's HSD test.\n",
    "\n",
    "2. Bonferroni Correction:\n",
    "   - The Bonferroni correction adjusts the significance level for multiple comparisons to maintain an overall family-wise error rate. It divides the desired significance level (e.g., 0.05) by the number of pairwise comparisons. If the p-value for a pairwise comparison is less than the adjusted significance level, then that comparison is considered significant.\n",
    "   - Example: If you have conducted multiple t-tests to compare the means of different groups, the Bonferroni correction can be applied to control the overall Type I error rate.\n",
    "\n",
    "3. Scheffé's Test:\n",
    "   - Scheffé's test is a more conservative post-hoc test that controls the family-wise error rate for all possible comparisons. It is suitable for situations where you have a smaller sample size and want to make a large number of comparisons.\n",
    "   - Example: In a study with multiple treatment groups and a small sample size, you can use Scheffé's test to compare the means of all groups.\n",
    "\n",
    "4. Dunnett's Test:\n",
    "   - Dunnett's test is used when you have a control group and want to compare the means of other treatment groups against the control group mean. It controls the Type I error rate for the multiple comparisons while taking into account the control group comparison.\n",
    "   - Example: In a medical trial, you have a control group receiving a placebo and several treatment groups receiving different doses of a drug. Dunnett's test can be used to compare each treatment group to the control group.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c89971-d953-48e7-91bd-e3e01f6a9433",
   "metadata": {},
   "source": [
    "# Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327073ee-aa44-464e-ade4-b2d7af221e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 41.423629324341206\n",
      "p-value: 7.0525624646654424e-15\n",
      "There is a significant difference in the mean weight loss between the diets (p < 0.05).\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "diet_A = [2, 3, 4, 5, 2, 3, 4, 3, 2, 1, 2, 4, 3, 3, 2, 3, 1, 2, 3, 4, 5, 3, 2, 3, 4, 3, 2, 4, 5, 2, 3, 4, 3, 3, 2, 1, 2, 3, 4, 5, 3, 2, 3, 4, 3, 2, 4, 5]\n",
    "diet_B = [3, 4, 5, 6, 3, 4, 5, 3, 3, 2, 4, 5, 4, 4, 2, 4, 3, 3, 4, 5, 6, 4, 3, 4, 5, 4, 4, 5, 6, 3, 4, 5, 6, 4, 3, 2, 4, 5, 4, 4, 3, 2, 4, 5, 4, 4, 5, 6]\n",
    "diet_C = [4, 5, 6, 7, 4, 5, 6, 4, 3, 4, 5, 6, 5, 5, 3, 5, 4, 4, 5, 6, 7, 5, 4, 5, 6, 5, 5, 6, 7, 4, 5, 6, 7, 5, 4, 3, 5, 6, 5, 5, 4, 3, 5, 6, 5, 5, 6, 7]\n",
    "\n",
    "data = np.concatenate([diet_A, diet_B, diet_C])\n",
    "\n",
    "\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in the mean weight loss between the diets (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference in the mean weight loss between the diets (p >= 0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af9df5-77e9-4d5f-81e5-6415dfd0b28e",
   "metadata": {},
   "source": [
    "# Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f78421a-8892-418f-ba5f-16e9d3306684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         sum_sq    df          F        PR(>F)\n",
      "Software             115.266667   2.0  64.037037  2.387112e-10\n",
      "Experience             0.133333   1.0   0.148148  7.037012e-01\n",
      "Software:Experience    1.666667   2.0   0.925926  4.098595e-01\n",
      "Residual              21.600000  24.0        NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Software': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C'],\n",
    "    'Experience': ['Novice', 'Experienced'] * 15,\n",
    "    'Time': [10, 12, 8, 9, 13, 14, 11, 10, 9, 11, 14, 13, 12, 11, 10, 9, 15, 14, 11, 12, 10, 11, 15, 16, 12, 10, 9, 10, 15, 14]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "formula = 'Time ~ Software + Experience + Software:Experience'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa1432d-9ec6-4bee-ad51-10d1a1ceef7e",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "\n",
    "- If the p-value for the main effect of software is less than the chosen significance level (e.g., 0.05), it indicates that there is a significant difference in the average time to complete the task across the three software programs.\n",
    "- If the p-value for the main effect of experience level is less than the chosen significance level, it indicates that there is a significant difference in the average time to complete the task between novice and experienced employees.\n",
    "- If the p-value for the interaction effect between software and experience level is less than the chosen significance level, it indicates that there is a significant interaction effect between the software programs and experience level. This means that the effect of software on the time to complete the task may be different for novice and experienced employees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d528a875-5015-470f-8b37-470a0e2864ea",
   "metadata": {},
   "source": [
    "# Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f37f1fa-da2d-422c-a0bb-490017ae7180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test:\n",
      "t-statistic: -20.366190118761846\n",
      "p-value: 5.41551424565112e-37\n",
      "There is a significant difference in test scores between the control and experimental groups (p < 0.05).\n",
      "\n",
      "Post-hoc test (Tukey's HSD):\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental      8.1   0.0 7.3107 8.8893   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "control_group = [75, 80, 85, 78, 82, 79, 81, 84, 76, 80, 77, 81, 83, 78, 82, 80, 79, 81, 85, 79, 82, 78, 81, 80, 84, 78, 79, 80, 83, 82, 79, 81, 80, 78, 79, 82, 80, 81, 78, 80, 82, 85, 79, 81, 80, 78, 79, 82, 80, 81]\n",
    "\n",
    "experimental_group = [85, 89, 91, 86, 90, 88, 92, 87, 86, 88, 90, 87, 89, 88, 87, 86, 89, 91, 88, 90, 87, 89, 90, 88, 86, 87, 90, 89, 91, 87, 89, 88, 90, 87, 86, 89, 90, 88, 92, 87, 89, 90, 88, 86, 87, 90, 89, 91, 88, 87]\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "\n",
    "print(\"Two-sample t-test:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in test scores between the control and experimental groups (p < 0.05).\")\n",
    "else:\n",
    "    print(\"There is no significant difference in test scores between the control and experimental groups (p >= 0.05).\")\n",
    "\n",
    "\n",
    "\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "\n",
    "all_data = np.concatenate([control_group, experimental_group])\n",
    "\n",
    "\n",
    "group_labels = ['Control'] * len(control_group) + ['Experimental'] * len(experimental_group)\n",
    "\n",
    "\n",
    "posthoc = mc.pairwise_tukeyhsd(all_data, group_labels)\n",
    "\n",
    "print(\"\\nPost-hoc test (Tukey's HSD):\")\n",
    "print(posthoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cafc6e-faaa-4d83-9f51-35f31e579adc",
   "metadata": {},
   "source": [
    "# Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc02f19-0aa9-4c82-8a80-783cd4653553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sum_sq    df         F    PR(>F)\n",
      "C(Store)    55629.6   2.0  0.367618  0.693452\n",
      "Residual  6582609.3  87.0       NaN       NaN\n",
      "\n",
      "Post-hoc test (Tukey's HSD):\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "=======================================================\n",
      "group1 group2 meandiff p-adj    lower    upper   reject\n",
      "-------------------------------------------------------\n",
      "     A      B     -5.8 0.9963 -175.1509 163.5509  False\n",
      "     A      C    -55.4 0.7162 -224.7509 113.9509  False\n",
      "     B      C    -49.6 0.7651 -218.9509 119.7509  False\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.multicomp as mc\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Store': ['A', 'B', 'C'] * 30,\n",
    "    'Sales': np.random.randint(1000, 2000, 90)\n",
    "}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "formula = 'Sales ~ C(Store)'\n",
    "model = ols(formula, data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n",
    "\n",
    "\n",
    "posthoc = mc.pairwise_tukeyhsd(df['Sales'], df['Store'])\n",
    "\n",
    "print(\"\\nPost-hoc test (Tukey's HSD):\")\n",
    "print(posthoc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
