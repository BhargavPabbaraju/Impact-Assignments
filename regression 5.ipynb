{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428477c9-29fd-420e-8263-7e2631fb525f",
   "metadata": {},
   "source": [
    "# Q1. What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "\n",
    "Elastic Net Regression is a hybrid regression technique that combines the properties of both Ridge Regression and Lasso Regression. It aims to address some of the limitations of these individual techniques while benefiting from their strengths. Like Ridge and Lasso Regression, Elastic Net is used for predicting a continuous dependent variable based on one or more independent variables.\n",
    "\n",
    "The primary difference between Elastic Net Regression and other regression techniques lies in its regularization term, which is a combination of both L1 (Lasso) and L2 (Ridge) regularization terms. This combination allows Elastic Net to handle situations where there are multiple correlated features or where the number of features is larger than the number of observations. Here are the key features and differences of Elastic Net Regression compared to Ridge and Lasso:\n",
    "\n",
    "1. **Regularization Term**:\n",
    "   - **Elastic Net**: The regularization term in Elastic Net combines both the L1 and L2 regularization terms. The overall penalty is a linear combination of the absolute values of coefficients (L1) and the squared magnitudes of coefficients (L2).\n",
    "   - **Ridge**: Uses only L2 regularization, which adds the sum of squared coefficients to the loss function.\n",
    "   - **Lasso**: Uses only L1 regularization, which adds the sum of absolute values of coefficients to the loss function.\n",
    "\n",
    "2. **Bias-Variance Trade-off**:\n",
    "   - **Elastic Net**: By incorporating both L1 and L2 penalties, Elastic Net can achieve a balance between Ridge and Lasso. It can handle multicollinearity and feature selection simultaneously.\n",
    "   - **Ridge**: Primarily reduces the impact of high-variance coefficients while keeping all features.\n",
    "   - **Lasso**: Drives some coefficients exactly to zero for feature selection but might not handle multicollinearity well in situations with high correlation among features.\n",
    "\n",
    "3. **Feature Selection and Shrinkage**:\n",
    "   - **Elastic Net**: Combines the benefits of feature selection (Lasso) and coefficient shrinkage (Ridge). It can exclude irrelevant features while also reducing the impact of multicollinearity.\n",
    "   - **Ridge**: Doesn't perform explicit feature selection and shrinks coefficients towards zero without necessarily making them exactly zero.\n",
    "   - **Lasso**: Performs feature selection by driving some coefficients to exactly zero, effectively excluding features.\n",
    "\n",
    "4. **Regularization Parameter Tuning**:\n",
    "   - **Elastic Net**: In Elastic Net, you have two tuning parameters: one controls the balance between L1 and L2 regularization, and the other controls the overall strength of the regularization.\n",
    "   - **Ridge**: Has one tuning parameter (λ or alpha) to control the strength of regularization.\n",
    "   - **Lasso**: Has one tuning parameter (λ or alpha) to control the strength of regularization.\n",
    "\n",
    "5. **Optimal Parameter Selection**:\n",
    "   - **Elastic Net**: The optimal combination of the two tuning parameters is typically chosen using cross-validation.\n",
    "   - **Ridge** and **Lasso**: The optimal tuning parameter is chosen similarly using cross-validation.\n",
    "\n",
    "Elastic Net Regression is especially useful when dealing with high-dimensional data, multicollinearity, and situations where both feature selection and coefficient shrinkage are desired. It provides a flexible approach that combines the strengths of Ridge and Lasso, making it a powerful tool for regression tasks with complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127df532-10eb-41bd-aa8d-0f5e5b75541a",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n",
    "\n",
    "Choosing the optimal values of the regularization parameters for Elastic Net Regression involves a process similar to the one used for Ridge and Lasso Regression. Elastic Net has two regularization parameters: \"α\" and \"λ\" (or \"alpha\" and \"lambda\"). \"α\" controls the balance between L1 (Lasso) and L2 (Ridge) regularization, while \"λ\" controls the overall strength of regularization. Here's a step-by-step guide to choosing optimal values for these parameters:\n",
    "\n",
    "1. **Split Data**: Divide your dataset into training and validation (or test) sets. The validation set will be used to evaluate the performance of models trained with different parameter combinations.\n",
    "\n",
    "2. **Choose α and λ Grids**: Select a range of values for both α and λ to explore. For α, values between 0 and 1 are common. For λ, you can use logarithmically spaced values, similar to Ridge and Lasso Regression.\n",
    "\n",
    "3. **Loop Over Parameter Combinations**: For each combination of α and λ, follow these steps:\n",
    "   a. Train an Elastic Net Regression model using the training set and the selected α and λ values.\n",
    "   b. Calculate the model's performance (e.g., Mean Squared Error) on the validation set.\n",
    "\n",
    "4. **Cross-Validation**: Similar to Ridge and Lasso, you can perform k-fold cross-validation to robustly evaluate different combinations of α and λ. Divide the training set into k subsets (folds), then train the Elastic Net model k times. In each iteration, one fold is used as the validation set, and the remaining k-1 folds are used for training. Calculate the average performance across all folds for each combination of α and λ.\n",
    "\n",
    "5. **Select Optimal Combination**: Choose the combination of α and λ that results in the best performance on the validation set or the average performance across cross-validation folds. The metric you choose (e.g., Mean Squared Error, R-squared, etc.) should align with your model's performance goals.\n",
    "\n",
    "6. **Refit Model**: After selecting the optimal combination of α and λ, refit the Elastic Net Regression model using the entire training set (not just the training subset used during cross-validation) and the chosen parameters.\n",
    "\n",
    "7. **Evaluate on Test Set**: Once the model is refitted, evaluate its performance on a separate test set that was not used for model selection or training. This provides an unbiased estimate of the model's generalization performance.\n",
    "\n",
    "8. **Grid Search**: If computational resources permit, you can perform a grid search over a wide range of parameter combinations to ensure comprehensive exploration of the parameter space.\n",
    "\n",
    "Python's scikit-learn library provides tools like `ElasticNetCV` and `GridSearchCV` that simplify the process of cross-validation and hyperparameter tuning for Elastic Net Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ab175-4fe3-461c-bd4d-7ef7290d9b97",
   "metadata": {},
   "source": [
    "# Q3. What are the advantages and disadvantages of Elastic Net Regression?\n",
    "\n",
    "Elastic Net Regression combines the strengths of both Ridge Regression and Lasso Regression while addressing some of their individual limitations. However, like any technique, Elastic Net has its own set of advantages and disadvantages. Here's an overview:\n",
    "\n",
    "**Advantages of Elastic Net Regression:**\n",
    "\n",
    "1. **Feature Selection and Shrinkage**: Elastic Net can simultaneously perform feature selection by driving some coefficients to zero (like Lasso) and shrinkage towards zero (like Ridge). This helps in reducing overfitting and creating more interpretable models.\n",
    "\n",
    "2. **Handles Multicollinearity**: Elastic Net can handle multicollinearity better than Lasso. By incorporating L2 regularization, it can mitigate the instability associated with high correlation between features.\n",
    "\n",
    "3. **Flexibility in Regularization**: The parameter \"α\" in Elastic Net allows you to control the balance between L1 and L2 regularization. This flexibility lets you adjust the amount of feature selection and shrinkage based on the problem's characteristics.\n",
    "\n",
    "4. **High-Dimensional Data**: Elastic Net is well-suited for datasets with a large number of features relative to the number of observations. It can effectively handle high-dimensional data while avoiding overfitting.\n",
    "\n",
    "5. **Generalization Performance**: When the dataset has correlated features and the true model has a mix of relevant and irrelevant features, Elastic Net often outperforms Ridge and Lasso individually, leading to better generalization performance.\n",
    "\n",
    "**Disadvantages of Elastic Net Regression:**\n",
    "\n",
    "1. **Two Hyperparameters**: Elastic Net has two hyperparameters to tune: \"α\" and \"λ\" (or \"alpha\" and \"lambda\"). This increases the complexity of the hyperparameter tuning process.\n",
    "\n",
    "2. **Parameter Sensitivity**: The performance of Elastic Net can be sensitive to the choice of hyperparameters. Proper cross-validation is necessary to find the optimal values.\n",
    "\n",
    "3. **Computational Complexity**: Elastic Net involves solving an optimization problem with two types of regularization terms. This can lead to higher computational complexity compared to Ridge or Lasso individually.\n",
    "\n",
    "4. **Interpretability**: While Elastic Net can create more interpretable models than some other complex techniques, it might still have limitations in terms of interpreting the significance of specific coefficients, especially when many features are involved.\n",
    "\n",
    "5. **No Automatic Feature Scaling**: Just like Ridge and Lasso, Elastic Net might require feature scaling (standardization) before applying it to the data. Incorrect scaling can affect the performance and interpretation of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b3301-c07c-4ab9-993a-5e3bbf60deb2",
   "metadata": {},
   "source": [
    "# Q4. What are some common use cases for Elastic Net Regression?\n",
    "\n",
    "Elastic Net Regression is a versatile technique that can be applied to various regression tasks. It is particularly useful in situations where the dataset has high-dimensional features, multicollinearity, and where both feature selection and coefficient shrinkage are desired. Here are some common use cases for Elastic Net Regression:\n",
    "\n",
    "1. **Genomics and Bioinformatics**: In genetic studies, where there are often thousands of genetic markers (features) and a relatively small number of samples, Elastic Net can help select relevant markers while accounting for potential correlations among them.\n",
    "\n",
    "2. **Economics and Finance**: Elastic Net can be used for economic forecasting, such as predicting stock prices or macroeconomic indicators. It can handle a wide range of economic variables that might exhibit multicollinearity.\n",
    "\n",
    "3. **Marketing and Customer Analytics**: In marketing, when dealing with datasets containing multiple marketing channels and their impacts on customer behavior, Elastic Net can assist in identifying the most influential channels while considering potential dependencies.\n",
    "\n",
    "4. **Medical Diagnostics**: In medical diagnostics, where multiple medical tests or features are used to predict the presence of a disease, Elastic Net can help select the most relevant tests while handling correlations among them.\n",
    "\n",
    "5. **Text Analysis and Natural Language Processing (NLP)**: Elastic Net can be applied to text classification tasks where the dataset includes a large number of text features. It can help in feature selection and handling collinearities among words.\n",
    "\n",
    "6. **Environmental Science**: In environmental studies, Elastic Net can predict environmental factors based on a variety of correlated features, such as pollutants, weather conditions, and geographical variables.\n",
    "\n",
    "7. **Image Analysis and Computer Vision**: Elastic Net can be used for image analysis tasks where features are extracted from images. It can help in selecting meaningful features while considering potential feature correlations.\n",
    "\n",
    "8. **Social Sciences and Survey Data**: In social science research, when dealing with survey data that includes multiple interrelated questions, Elastic Net can help in identifying key survey questions while accounting for correlations.\n",
    "\n",
    "9. **Industrial Processes**: In industries, Elastic Net can model relationships between various process parameters to predict product quality or process efficiency while handling possible interdependencies.\n",
    "\n",
    "10. **Energy Forecasting**: In energy consumption or production forecasting, Elastic Net can incorporate multiple factors, like weather conditions, historical data, and economic indicators, while considering feature interdependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d02a1a-3d55-4ab1-81b1-c51dda7a9fa1",
   "metadata": {},
   "source": [
    "# Q5. How do you interpret the coefficients in Elastic Net Regression?\n",
    "\n",
    "Interpreting the coefficients in Elastic Net Regression is similar to interpreting coefficients in other linear regression techniques, but there are some considerations due to the combined L1 (Lasso) and L2 (Ridge) regularization terms. The coefficients represent the relationship between the independent variables (features) and the dependent variable (target), while also accounting for the impact of the regularization terms. Here's how you can interpret the coefficients in Elastic Net Regression:\n",
    "\n",
    "1. **Magnitude of Coefficients**: The magnitude of a coefficient indicates the strength and direction of the relationship between the corresponding feature and the target variable. A positive coefficient suggests a positive correlation, meaning that an increase in the feature's value is associated with an increase in the predicted value of the target. A negative coefficient indicates a negative correlation, implying that an increase in the feature's value leads to a decrease in the predicted target value.\n",
    "\n",
    "2. **Coefficient Significance**: Just like in ordinary linear regression, the significance of a coefficient in Elastic Net Regression can be determined by its p-value. A low p-value indicates that the coefficient is statistically significant and likely not due to random chance. It means that the corresponding feature has a significant impact on the target variable.\n",
    "\n",
    "3. **L1 Regularization (Lasso) Impact**: The L1 regularization term in Elastic Net can drive some coefficients to exactly zero, resulting in automatic feature selection. If a coefficient is exactly zero, it means that the corresponding feature has been deemed irrelevant by the model for predicting the target.\n",
    "\n",
    "4. **L2 Regularization (Ridge) Impact**: The L2 regularization term in Elastic Net contributes to shrinking the magnitudes of coefficients. This helps prevent overfitting and stabilizes coefficient estimates, especially for correlated features.\n",
    "\n",
    "5. **Magnitude Comparison**: When interpreting the magnitudes of coefficients, it's important to consider their scales and units. Standardizing the features (scaling them to have zero mean and unit variance) can help in comparing the relative importance of coefficients.\n",
    "\n",
    "6. **Interactions and Domain Knowledge**: Similar to other regression models, Elastic Net assumes linear relationships between features and the target. If you suspect interactions or non-linear effects, you might need to include interaction terms or polynomial features in your model. Additionally, domain knowledge is crucial for understanding the practical implications of the coefficients.\n",
    "\n",
    "7. **Regularization Strength (α and λ)**: The two regularization parameters, \"α\" and \"λ,\" control the strength of the L1 and L2 regularization terms, respectively. The choice of these parameters affects the model's complexity, bias-variance trade-off, and the number of selected features.\n",
    "\n",
    "8. **Coefficient Stability**: In some cases, due to the combined effect of L1 and L2 regularization, the coefficients might be more stable than in pure Lasso or Ridge Regression. However, it's important to note that the interpretation of coefficients might still be challenging when dealing with highly correlated features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361a9c5-8549-436c-8a58-9be7384c993e",
   "metadata": {},
   "source": [
    "# Q6. How do you handle missing values when using Elastic Net Regression?\n",
    "\n",
    "Handling missing values is an important preprocessing step when using any regression technique, including Elastic Net Regression. Missing values can lead to biased and inaccurate model results. Here are some strategies to handle missing values when using Elastic Net Regression:\n",
    "\n",
    "1. **Imputation with Mean, Median, or Mode**: One common approach is to replace missing values with the mean, median, or mode of the respective feature. This can be a simple way to fill in missing values while minimizing the impact on the overall distribution of the data. Choose the imputation method based on the nature of the data and the extent of missingness.\n",
    "\n",
    "2. **Imputation with Predictive Models**: For features with missing values, you can use other features as predictors to build predictive models and predict the missing values. This can be achieved using techniques like linear regression, decision trees, or k-nearest neighbors.\n",
    "\n",
    "3. **Use of Indicator Variables**: You can create an additional binary indicator variable that takes the value 1 when the original variable is missing and 0 otherwise. This way, the model can capture the effect of missingness as a separate category.\n",
    "\n",
    "4. **Deletion of Rows**: If the missing values are relatively few and randomly distributed, you might choose to simply remove the rows with missing values. However, this approach can lead to loss of information and bias if the missing values are not random.\n",
    "\n",
    "5. **Feature Engineering**: Sometimes, missing values themselves might contain important information. For example, a missing value in a survey question might indicate that the respondent did not answer that question. You can create a binary feature indicating the presence of missing values and include it in the model.\n",
    "\n",
    "6. **Imputation with Machine Learning Models**: Advanced imputation techniques involve using machine learning models like k-nearest neighbors, decision trees, or regression to predict missing values based on other features. This can provide more accurate imputations, especially when relationships between features are complex.\n",
    "\n",
    "7. **Multiple Imputation**: Multiple Imputation is a sophisticated technique that involves creating multiple imputed datasets, each with slightly different imputed values. You then run the regression on each imputed dataset and combine the results to obtain more accurate estimates and uncertainty intervals.\n",
    "\n",
    "8. **Domain Knowledge**: In some cases, domain knowledge can help in determining appropriate imputation strategies. Understanding why values are missing can guide the imputation approach.\n",
    "\n",
    "When using Elastic Net Regression, the chosen imputation strategy should be aligned with the characteristics of the data and the goals of the analysis. It's also important to remember that the imputation process can introduce biases if not done carefully, so it's advisable to assess the impact of different imputation methods on the model's performance and validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96240cb1-996a-44c3-be84-b3c69b182512",
   "metadata": {},
   "source": [
    "# Q7. How do you use Elastic Net Regression for feature selection?\n",
    "\n",
    "Elastic Net Regression is well-suited for feature selection due to its ability to simultaneously perform feature selection and coefficient shrinkage. The L1 (Lasso) regularization term in Elastic Net can drive some coefficients to exactly zero, effectively excluding features from the model. Here's how you can use Elastic Net Regression for feature selection:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Handle missing values and ensure that your data is properly preprocessed, including feature scaling if necessary.\n",
    "\n",
    "2. **Split Data**:\n",
    "   - Divide your dataset into a training set and a validation (or test) set. The validation set will be used to evaluate the performance of models with different sets of features.\n",
    "\n",
    "3. **Choose α and λ**:\n",
    "   - Select values for the α (balance between L1 and L2 regularization) and λ (strength of regularization) parameters. You can choose a range of values for each parameter.\n",
    "\n",
    "4. **Loop Over Parameter Combinations**:\n",
    "   - For each combination of α and λ, follow these steps:\n",
    "     a. Train an Elastic Net Regression model using the training set and the selected α and λ values.\n",
    "     b. Retrieve the coefficients of the model.\n",
    "\n",
    "5. **Rank Features**:\n",
    "   - Rank the features based on their corresponding coefficients' magnitudes. Larger absolute coefficients indicate stronger associations between features and the target variable.\n",
    "\n",
    "6. **Select Features**:\n",
    "   - You can choose a certain number of top-ranked features or set a threshold for coefficient magnitudes to determine which features are considered relevant.\n",
    "\n",
    "7. **Refit Model**:\n",
    "   - After selecting the desired features, refit the Elastic Net Regression model using the selected features and the optimal values of α and λ.\n",
    "\n",
    "8. **Evaluate on Validation/Test Set**:\n",
    "   - Evaluate the performance of the model with the selected features on the validation or test set. This step ensures that the feature selection process doesn't lead to overfitting or poor generalization.\n",
    "\n",
    "9. **Fine-Tuning and Cross-Validation**:\n",
    "   - If you're not satisfied with the initial feature selection results, you can further fine-tune the values of α and λ using cross-validation. This process helps ensure that you're choosing the best combination of regularization and feature selection.\n",
    "\n",
    "10. **Interpret Results**:\n",
    "    - Interpret the final model's coefficients, keeping in mind that features with non-zero coefficients are the selected features that contribute to the model's predictions.\n",
    "\n",
    "It's important to note that the choice of α and λ parameters greatly affects the feature selection process. Higher values of α tend to result in more features being excluded due to stronger L1 regularization. On the other hand, smaller values of α allow for a mix of L1 and L2 regularization, which might result in fewer features being excluded.\n",
    "\n",
    "Python libraries like scikit-learn provide tools like `ElasticNetCV` for performing Elastic Net Regression with cross-validation, making it easier to automate the process of finding the optimal regularization parameters and performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddecf3e-c2f7-4e3c-b8d6-1a848a7bbcd4",
   "metadata": {},
   "source": [
    "# Q8. How do you pickle and unpickle a trained Elastic Net Regression model in Python?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5f2ef7-47b7-4b82-96a8-e2fe787c68d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Load example dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train an Elastic Net model\n",
    "model = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Serialize and save the trained model to a file\n",
    "with open('elastic_net_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b4c134-49fc-4527-813f-19d2b55c6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickled model from the file\n",
    "with open('elastic_net_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09350118-eb1c-480e-92c3-5170ef321d63",
   "metadata": {},
   "source": [
    "# Q9. What is the purpose of pickling a model in machine learning?\n",
    "\n",
    "Pickling a model in machine learning refers to the process of serializing (converting to a byte stream) a trained machine learning model into a format that can be stored in a file or transmitted over a network. The purpose of pickling a model is to save its state, including learned parameters and internal settings, so that it can be later restored and used for various purposes without the need to retrain the model from scratch. Here are some common purposes of pickling a model in machine learning:\n",
    "\n",
    "1. **Persistence**: Trained machine learning models are the result of potentially time-consuming computations. Pickling allows you to save the model's state to disk, ensuring that you can reload and use the model without the need to retrain it every time.\n",
    "\n",
    "2. **Deployment**: After training a model on one machine or environment, you can pickle it and deploy it to other systems or production environments without the need to carry over the entire training process.\n",
    "\n",
    "3. **Sharing**: Pickled models can be easily shared with collaborators or deployed to production servers, enabling consistent model predictions across different environments.\n",
    "\n",
    "4. **Caching**: If a model is used to generate predictions frequently, pickling allows you to save a copy of the trained model in memory to avoid repeated loading and initialization.\n",
    "\n",
    "5. **Versioning**: By pickling models along with relevant metadata, you can ensure that the exact version of the model used for a particular task can be reproduced later, even if the original training code or data changes.\n",
    "\n",
    "6. **Scalability**: In distributed systems, training models might be done on one machine and then distributed to other machines for inference. Pickling allows easy transfer of the model between machines.\n",
    "\n",
    "7. **Web Applications**: Pickling is useful when integrating machine learning models into web applications or APIs. The model can be loaded from a pickled file and used to provide predictions to users in real-time.\n",
    "\n",
    "8. **Offline Testing**: You can pickle a model after training and use it for offline testing and debugging without the need to re-run the entire training pipeline.\n",
    "\n",
    "9. **Ensemble Learning**: In ensemble learning, you can pickle individual base models (classifiers or regressors) and then combine them later to create ensemble models like Random Forests or Gradient Boosting.\n",
    "\n",
    "10. **Experiment Reproduction**: Pickling models along with relevant hyperparameters and feature engineering steps allows you to reproduce experiments and results precisely, even after some time has passed.\n",
    "\n",
    "While pickling provides many benefits, it's important to consider a few points:\n",
    "\n",
    "- Models pickled using one version of a library might not be compatible with another version. Ensure consistency in library versions.\n",
    "- Pickling large models with substantial amounts of data can result in large pickled files, which might affect storage and transfer efficiency.\n",
    "- Pickled models are binary files and can't be easily inspected or edited like text-based formats.\n",
    "\n",
    "Overall, pickling models is a convenient and efficient way to save and share trained models in machine learning, facilitating deployment, testing, and reproducibility.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
